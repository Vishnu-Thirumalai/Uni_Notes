\section{AI and Humans}

\subsection{The Internet}
Early pioneers of the internet thought it would be a global melting pot, where people could learn/share knowledge/debate. Instead, we've got fake news, corporations spreading their influence, and impersonations taking over.\\

\subsubsection{Filter Bubbles}
Websites use personalised searches to restrict and filter what users see, on the assumption that what you've already seen is what you want to continue to see. This limits people from outside/contradicting influence, and confirms/solidifies your views as you don't see countering arguments.\\
This has led to a rise in \emph{belief polarisation}: people only believe their own views, and don't see the opposition. This causes them to be less willing to change and tend towards more extreme views. Without divers perspectives, we become less tolerant and more radical.

\subsubsection{Echo Chamber}
The phenomenon where people only get information from like-minded people: by only associating with certain groups, all other views are filtered out. People in these groups bounce claims/opinions off each other, and repetition strengthens these. \\
Since these people are in a group, they often feel safer to say harsher/more controversial things,  and trust opinions from like-minded people: people are more likely to trust people than opinions from the news or media.\\
Echo chambers segregate people into particular directions: this is especially prevalent online, with political echo chambers being of particular note.

\subsection{Argumentative Theory of Reasoning}
This argues that reasoning evolved for social reasons, not just for individuals: to allow arguments and group decisions. This is opposed to the Cartesian view that reasoning is to critically examine/correct ones own beliefs (which has been shown by psychology to not be perfect as humans make mistakes, and often don't come to a correct internal reasoning). \\

It argues that humans evolved to be social and this required communication, to be able to pass lessons/ideas to others. For communication to work, the listener has to be able to discern truth/lies, otherwise the speaker has all the power. The listener needs \emph{epistemic vigilance} (careful about incoming knowledge), understanding the arguments for/against what the speaker says and weigh them up. \\

This forces speakers to provide arguments for what they say (otherwise nobody would listen), and the listeners look for counter arguments: which is why humans have a \emph{conformation bias} - while alone, they only look for reasons to support what they believe, and don't look for counter arguments. \\

This also explains why people tend to make decisions that can be justified with reasons: they're less likely to be countered by others. This is opposed to the best decisions or rational decisions: these are more likely to be criticised if we can't justify them. 

\subsubsection{Formation of Echo Chambers}
Traditionally, we've had to interact with a large range of options since we've had limited access to information. e.g. newspapers, local communities. Social media/ the internet has given us more information, and filtering algorithms have amplified our instincts to only pay attention to information that strengthen our arguments/beliefs while shielding us from counter-arguments.

\subsection{Dialogue}
Engaging in dialogue with groups has shown to lead to better decisions from the group, as opposed to a single individual - this is because people are forced to engage with counterarguments/challenges. More information is exchanged, and people are given exposure to different views. This may prevent people from being isolated in belief bubbles: however, it might force defensive reactions from people or cause them to stop engaging altogether.\\

\subsubsection{Dialogue Scaffolding}
By using computational models of dialogue, we can scaffold human-human reasoning: it can force critical discussions and clear up intentions in dialogue. e.g. if person A gives multiple points and B disagrees, the AI can help/force the people to clarify which points they disagree with and why.\\ 

AI could trawl the web for argument, modularise them into for/against, and learn to debate on its own - therefore how to regulate debates into a computational model as well. An example of this is the IBM Project Debater: an AI system that digests texts and structures a speech to rebut opponents in real time. It can contribute to discussions by providing evidence-based arguments, as opposed to introducing bias/emotions

\subsubsection{Countering Echo Chambers}
When exposed to opposing views, people tend to become more rigid in their views in an us vs. them mentality. This is especially true of those from a  more intellectual background. Countering this requires a change in how we interact with information: rather than just searching for what we want, we engage in a dialogue about the information as well - this helps with developing critical reasoning skills\\

There are models in use for educational contexts: people often learn/improve their knowledge base through dialogue, refutation and understanding. e.g. doctors on ward rotations. Having an AI that can reason/debate/argue would be a huge help to education, teaching children to reason and debate from an early age

\subsubsection{Dialogues for Moral Reasoning}
Any moral/ethical decision can be framed in terms of maximising utility/happiness. Happiness is subjective and based on human experience, so any complex moral decision requires human inputs. A combination of AI's epistemic knowledge/causal reasoning and human's subjective/moral reasoning improves any decisions made - they can be combined via dialogue (such as argumentation frameworks). One example is MIT's Moral Machine, that combines human choices with utilitarian factors to decide who self-driving cars would kill if they had to choose.

\subsection{Surveillance Capitalism}
Capitalism takes what is freely available and makes it commodities.e.g. labour, nature, education, land. As soon as something can be bought or sold, it changes the dynamic around it.\\

Surveillance capitalism started in the early 2000s, when google started to sell their data on peoples searches - this data can be used to predict what type of person you are and your future actions. It can also show what factors can be used to change your future behaviour, without you noticing it. \\

At its most basic level, the data can be used to filter towards things that you like/want, and remove useless things that you won't interact with: creating a filter bubble. Another example was the mobile game Pokemon Go, which had shops pay them to place virtual beacons in shops, so more people would go into visit them: physical manipulation from the virtual world.\\

Such examples may seem small, but when this manipulation can affect political choices or the notion of free will it starts to become control. Not only do we have less \emph{serendipity} (chance to encounter new things we like/challenge our beliefs), which fixes us to a snapshot of our likes/beliefs, but it further undermines free will - though everything is already predetermined, we have the illusion of choice. With enough data on a person, corporations can manipulate the background processes to force you to make a certain choice, even if we don't realise it. \\

An example is from Facebook: they used data to model teenagers' emotional states and determine when they were feeling vulnerable, then set them targeted ads to cause them to indulge in retail therapy (owning things makes people feel better). This was promoted as a way for companies to increase revenue, scientific research shows retail therapy does little/nothing to help.\\

Data can be used for good (e.g. to personalise things), but there need to be boundaries on what companies can do with our data and who they can sell it to.

\subsection{AI's Effect on Jobs}
Many (McKinsey global institute, Oxford Economics) say that lots of clerical work and manufacturing jobs (i.e. jobs that have a fixed routine) will be lost to AI and robots, especially in the short term. The long term effects are still unclear, with experts divided but generally saying more jobs will be created overall. Even further long term effects (i.e. AGI) hasn't been discussed as much, but could be a net loss in jobs.

\subsubsection{Ethical Concerns For/Against}
\begin{enumerate}
    \item Many jobs can be made safer with the use of AI. e.g. self driving cars
    \item Universal Basic Income could be made available with the rise in economic growth
    \item Even with UBI, many humans rely on work as a meaning/purpose for life: it's possible they can take on more creative activities to compensate
    \item There could be a cognitive decline, as humans don't use their skills (e.g. people have stopped using maps since GPS became widespread)
\end{enumerate}

\subsection{AI and Ethics}
As mentioned in \ref{subsubsec:Coop_Reinforcement_Learning}, AI and humans can collaborate to maximise the AI's reward function and ensure it follows human values. When humans don't know the right answers, AI can extrapolate to learn and help humans decide what is right and wrong. \\
To determine true happiness maximisation, empathy is required: AI can help humans with this by helping humans understand each other. An AI-powered virtual reality could be the ultimate 'walk a mile in the other person's shoes before you judge them'.

\subsubsection{Availability of AI}
If AI is only available to the rich/powerful, rather than their benefits being shared with all of humanity (as in the Asimolar Principles), this will further increase the already growing inequality. This has already started with how large companies use AI to get further ahead than their competitors

\subsubsection{Conscious AI}
If AI become conscious, they become moral agents with the ability to feel happiness/suffering - we would feel the need to treat them with rights as we should all other moral agents (this is especially true with speed superintelligence - a minute of suffering for us would be subjectively lifetimes to them). \\

Determining consciousness is a problem: functionalism (if the AI acts conscious, that's enough) is our current solution. Humans (especially children) naturally anthropomorphise everything, attributing feelings and consciousness even if they're not present. There are also strong moral/commercial motivations to make AI/robots more human-like: they more easily integrate into human society/culture, and people will more readily accept/use AI robots if they're more human (e.g. elderly care robots, sex robots)\\

\subsubsection{Downsides of Human-like AI}
\begin{itemize}
    \item However, most robots/AI will be treated as sub-human (slaves or objects): this could degrade our attitudes towards humans, also treating others as sub-human.
    \item Relying on robots for morals and ethics could cause or own to degrade (like cognitive functions decline) - we'd rely on them to tell us what the right thing is
    \item Humans interacting with robots will have the power in the relationship: this will affect how they interact/empathise with other humans (especially in children, who are more likely to anthromorphise things)
\end{itemize} 