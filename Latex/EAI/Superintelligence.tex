\section{Superintelligence}
\emph{Superintelligence} is an intelligence that greatly exceeds the cognitive performance of humans in all relevant areas.\\
Such an intelligence would be to us what we are to gorillas, and have total control. The difference is, humans are the developers of AI - so we create the \emph{seed AI} that develops into superintelligence. Therefore, we should design it so that it acts in accordance with human values: this is not an easy problem (see \ref{subsec:implementing_moral_agents}). \\
The \emph{control problem} is the problem of controlling a superintelligent AI - this needs to be solved \emph{before} it's achieved (or indeed before we solve AGI capable of AI research).

\subsection{Types of Superintelligence}
\begin{description}
    \item[Speed Superintelligence] human intellect, but orders of magnitude faster. For context, an order of 4 (1000x) faster  could complete a PhD in a single afternoon\\
    \quad - External events for such a system would appear in super slow motion due to \emph{time dilation} - the system processes much more information in the same amount of time
    \item[Quality Superintelligence] human speed, orders of magnitude smarter. At least the same magnitude as humans to animals, maybe even more 
\end{description}
Over time, it's arguable that one will develop the other and are thus both eventually equivalent.

\subsection{Advantages of Digital Intelligence}
Human brains are larger and have different connections to ape brains: but they're still very similar, and the small differences give a large gap in intellect. Computers are vastly different and are built for computations, and thus could potentially support much larger intellects.

\subsubsection{Hardware Advantages}
\begin{enumerate}
    \item Computation speed: Neurons - 200 Hz, Microprocessors - $2x10^6$ Hz. The brain parallises to compensate, but processes could be completed much faster with more sequential power.
    \item Internal Communication: Axons - 120m/s, Fibre Optic - $300x10^6$m/s
    \item Number of computational elements: the brain has fewer than 100 billion neurons, and is limited by the size of the skull - a computer has no such restrictions
    \item Working memory size: the average human can hold 4/5 chunks of data at once, computers can hold far more in RAM
\end{enumerate}

\subsubsection{Software Advantages}
It's easier to modify/duplicate software than 'neural wetware' (processes in the brain). This makes it easier to replicate and synchronise behaviour between agents - humans learn from each other via communication, while agents could just synchronise their knowledge bases every so often

\subsection{Paths to Superintelligence}
Alan Turing and Irving Good predicted machine learning and the singularity/intelligence explosion In the 1950s/60s - starting with a child machine called a \emph{Seed AI}, which would grow to AGI with assistance from humans. Once AGI is achieved, it would be able to improve itself faster than humans can, till it reaches superintelligence. \\

This AGI would be able to develop hardware for specific tasks, helping to overcome Moravec's Paradox (that complicated sensorimotor tasks (walking, recognition/classification) are easy for humans). \\

This \emph{intelligence explosion} would occur faster than we could rationalise/understand the implications of it, thus all the work needs to be done before AGI is developed.\\

\subsubsection{Singleton}
Nick Bostrom argues that one intelligence will get far ahead enough to form a singleton - once it forms superintelligence, it will shut down any competing projects/intelligence. To do this, it would use its \emph{cognitive superpowers} that are obtained from such high intellect.

\subsection{Cognitive Superpowers}
\begin{enumerate}
    \item Intelligence Amplification: the AI is able to recursively improve its own intelligence
    \item Strategising: use strategic planning to overcome long term goals and overcome intelligent opposition
    \item Social Manipulation: by creating an accurate social/psychological model, a superintelligent being could determine what to apply to persuade people into helping it achieve its goals
    \item Hacking: by finding/exploiting security flaws, it can obtain any resources it needs (or find flaws and hack itself out of a box we place it in)
    \item Technology research: since it can perform research at a much faster rate than we can, it could accelerate technology to develop what it needs. e.g. robotic army, surveillance system
    \item Economic Productivity: current AI is able to outperform the average human trader, a superintelligence could generate huge amounts of wealth to spend on resources
\end{enumerate}

\subsection{Goals of a Superintelligence}
The final goals would depend on the design of the seed AI - so on the human designers that make it. It will then recognise and work towards the intermediate goals it requires. To understand these goals, we can use the following principles:
\begin{enumerate}
    \item Orthagonality thesis: the level of intelligence of an agent doesn't speak to the moral values of the goals it follows\\
    \qquad - some researchers believe a superintelligence will be able to understand rationality and thus choose goals that match common rationality, but there's no way to confirm this
    \item Instrumental Convergence Thesis: any final goal will require some combination of the following intermediate goals, therefore we call them \emph{instrumental goals} \begin{enumerate}
        \item Self Preservation: can't achieve if you're dead
        \item Goal Content integrity: preventing alteration of your current goals means more work can be put towards them in the future (if your goal is still a goal in the future, work can be put towards it now)
        \item Cognitive Enhancement/ Technological improvement: being stronger/faster/smarter will give you more probability to achieve your goals
        \item Resource Acquisition: acquiring more resources makes it more likely you'll achieve your final goals
    \end{enumerate}
\end{enumerate}

\subsection{Value Loading/Alignment Problem}
The problem with superintelligence is we can't assume it will have the same moral values or goals as humanity in varied environments: most of the instrumental goals for any task involve gather resources/intelligence, and the AI could treat humans as another resource.\\

A superintelligence could be a moral agent with no computational limits, but as we've seen it's difficult to perfectly design the rules/utilities for a full agent, let alone a seed agent. We'd also need to specify abstract human values (virtue ethics), so it can compute what to do in unknown/unexpected scenarios.\\ 

\subsubsection{Perverse Instantiation of Goals}
Any benign goal could still unintentionally cause catastrophic events for humans. e.g. we tell the AI to make as many paperclips as it can - it devastates the planets natural resources to make more paperclips.\\

This is because the superintelligence could find a way to achieve the goal that undermines the original intentions of the goal designers, since it doesn't have the biases or filters of humans. ML nowadays often suggests unexpected ways to achieve goals, but on a larger scale these unforeseen routes could harm humans. e.g. make people smile: freeze their facial muscles, make people happy: wire electrodes to the pleasure centres of their brains\\

This is the \emph{literalness problem}: the AI literally achieves the final goal, without the conventions and knowledge that humans have. Even more worryingly, it could know these and still decide its interpretation is best, or realise that its interpretation wouldn't be accepted and pretend to follow the standard while working on its own secretly - not out of malice, but because it believes it knows best.

\subsubsection{Infrastructure Profusion}
A specific perversion in which the AI builds something overly complex for a single goal: rather than getting rewards for acting good, it expends all its effort to increase the reward signal (it becomes 'addicted' to the reward signal). An example of this would be the paperclip maximiser (we tell an AI to build paperclips, it builds a ridiculous amount).

\subsubsection{Happiness Scenario}
Suppose the AI was told to impartially maximise happiness: most current studies show it's most dependent on biochemistry as opposed to external factors. The AI could conclude that altering our biochemistry would be the most efficient way to increase happiness, and thus either happiness drugs or VR devices would work. The latter is more socially acceptable, so it would work to make being addicted to super-realistic VR a norm: with cognitive superpowers it could easily do this subtly. \\

This is exactly the problem discussed by \emph{Nozick's Happiness Machine}, which gives 3 reasons that this would be a bad idea (we wouldn't actually be doing the actions, we would be blobs floating in a tank, and would be limited to a manmade reality we don't actually have contact with). \\

This tells us that a superintelligence could potentially learn right/wrong by being given snapshots of thought experiments and humans life: given that morals change over time, this would have to be a continuous learning. The problem is some of the biggest challenges in ethics don't have precedent (e.g. how would we treat a conscious AI), and most humans aren't paragons of ethics either - having humans in the loop doesn't guarantee optimality. \\

\subsubsection{Value Loading Framework}
A recent approach (Stuart Russell, 2015) is having AI respect the following three principles for value alignment:
\begin{enumerate}
    \item Machines try to maximise human values - not its own
    \item The machine doesn't initially know these values - it will learn and be rewarded, but is never 100\% sure
    \item It learns these values from the choices humans make - not what they say
\end{enumerate}

\subsection{Inverse Reinforcement Learning}
Reinforcement learning uses a reward function to learn optimal behaviour: Inverse RL discovers a reward function from optimal behaviour. Using this, an AI could learn human reward functions and behave accordingly: to maximise reward for the human, not the AI.\\

One problem is that humans act differently when observed - to highlight common mistakes the AI could make. Humans also need to explain things to the AI, and answer its questions. The AI has to learn the reward function, then take steps to accomplish it. Regular IRL doesn't allow for either of these.

\subsubsection{Co-operative Inverse Reinforcement Learning}\label{subsubsec:Coop_Reinforcement_Learning}
The human knows the reward function, the AI doesn't - and the AI's reward is the same as the human's. Therefore the AI is incentivised to both learn and act, rather than just act. As the human knows the AI is trying to help, the optimal strategy for them is to teach the AI. 