\section{Applications of AI}

\subsection{Lethal Autonomous Weapon Systems (LAWS)}
These decide/start/carry out attacks all by themselves - there is no human in the loop from searching to attacking. These are still in early stages, but heavy work is going into them all over the world.

\subsubsection{Arguments for Banning}
\begin{enumerate}
    \item Development of such weapons is a race, so it's likely scientists will skip ethical/safety precautions
    \item Prevent overuse - these would be a lot cheaper than nukes, so mass proliferation/production. This in turn means more weapons on the black market, and these weapons can be hacked to remove any safeguards in place on their use.
    \item Less human deaths, so more countries will be willing to go to war. However, so far there hasn't been an increase in war with an increase in technology - it's all based on politics and diplomacy
    \item If a country with nukes doesn't have LAWS and get attacked, they might escalate with nukes
    \item Technological challenge: LAWS would be incredibly difficult to implement so they follow laws, respect international treaties, respond to unexpected situations, recognise soldiers vs. civilians \dots . There's no guarantee that these can be correctly solved in the short term.
    \item Takes Humans out of the loop - there is no empathy/sympathy in a machine, so can't accurately judge military gain/human harm or make humane decisions.\\
    \qquad - Are these necessary though? Virtue ethics vs. consequentialism\\
    \qquad - The conviction that machines can't make better decisions is a moral one - as culture evolves (e.g. driverless cars) it might be more acceptable
    \item Accountability: who would be to blame for LAWS' actions?\\
    \qquad - Is it worth sacrificing any potential benefits just because someone needs to be held accountable?
\end{enumerate}

\subsubsection{Arguments against Banning}
\begin{enumerate}
    \item Who's going to listen to the ban? Nations will continue to develop these technologies without consequences. However, previous bans have been successful in reducing certain types of weapons
    \item LAWS could be better than humans at discriminating soliders/civilians - most weapon systems are banned because they're indiscriminate
    \item LAWS could be better than humans overall - they stop once they've achieved their goal/costs outweigh the benefits, as they're not shackled by revenge/panic/emotions
\end{enumerate}

\subsubsection{Overall}
Most arguments against talk about the potential benefits and utilities - at the current rate of AI, we're nowhere close. \emph{Pragmatic Utilitarianism} might suggest that humans need humans to be in the loop when it comes to killing and war, even over any benefits. Semi-autonomous weapons (use humans for kill confirmations) could be a balance between the two. 


\subsection{AI in Healthcare}
AI is being used to improve healthcare across the board - from drug synthesis to identifying cancer cells in tissue. These systems can be used to support doctors, or even replace them in some cases. Given the large amount of medical data becoming available, AI-based systems are required to keep up.\\

A large example is IBM Watson, which takes in symptom data and mines it to give different diagnoses ranked by probability. Another is using robotics to assist in delicate surgeries.

\subsubsection{Ethical Concerns}
\begin{enumerate}
    \item Neural Networks are black boxes: we don't know how they identify things. However, given that it's nearly 100\% accurate should we care how we obtained the results (utilitarian approach - it only provides benefits)? \\
    \qquad - Experts could potentially explain the features that the AI finds, or explain specific images after they've been identified.
    \item Automation bias: if it's all done by the machine, the professionals become complacent and don't check. Again, this still provides net benefits
    
    \item What is the role of these systems? IBM Watson is marketed as a helping tool, not intended to take over the role of a doctor or used as the final check to make decisions. However due to this designation (As a helping service not a device), it's not currently subject to regulations
    \item Liability for doctors that don't correctly interpret/verify the outputs of such systems. 
    \item Liability as such systems increase the legal standard of care, so doctors that don't use such systems (or the systems are wrong) can be sued for negligence
    \item A system could contradict the doctor/existing medical standards, and we wouldn't know why - the doctor would need to justify their decision to follow/not follow the advice
    
    \item Informed consent: competent patients who can make voluntary must be given information on medical interventions and give their consent before they can take place. Not only the black box, but the doctor and patient both need to understand the information about the AI.\\
    \qquad - The doctor should at least state the division of responsibility between human/machine and potential harms
    \item Assignment of moral responsibility/legal liability: details of the machine need to be made clear at all stages  \\
    \qquad - Currently medical liability falls under tort law, which covers liability by the doctor/hospital/manufacturer
\end{enumerate}

\subsection{Professional Code of Ethics for R\&D of AI}
Professionals have power over their clients: ethical codes present misuse of this power and force them to cater for all clients (e.g. visually impaired public). For AI research this can be a problem as there is considerable commercial interests in AI, and there are many arguments that diversity among AI researchers is low. \\
AI itself poses a problem, in that AI might have power over researchers/developers and even they might not now how certain types of AI works (black box).

\subsubsection{Asimolar Principles}
A code of ethics developed by the Future of Life institute for AI researchers to follow  (can be viewed \href{https://futureoflife.org/ai-principles/}{here}):
\begin{enumerate}
    \item Research Issues:
    \begin{enumerate}
        \item AI research should be directed towards beneficial intelligence
        \item Investments in AI should also fund research on ensuring its beneficial use
        \item AI researchers should have a healthy exchange with AI policy makers
        \item Co-operation, trust and transparency between AI researchers
        \item Teams working on AI should work together to avoid corner-cutting on safety standards
    \end{enumerate}
    \item Ethics and Values:
    \begin{enumerate}
        \item AI systems should be safe and secure, and where possible this should be verifiable
        \item If the system goes wrong, it should be possible to know why
        \item \emph{If a system is involved in judicial decision making, its explanation should be able to be examined by a competent human authority}
        \item Designers/Builders are part of the moral implications of their use/misuse, and have the responsibility to shape them
        \item Highly autonomous AI systems should be designed to always align with human values
        \item AI systems should be designed and operated so as to be compatible with ideals of human dignity, rights, freedoms, and cultural diversity.
        \item People should have the right to manage the data they generate
        \item Applying AI to personal data shouldn't curtail people's liberties
        \item AI technologies should be used to benefit and empower as many people as possible
        \item The prosperity generate by AI should be shared with all
        \item Humans control whether to give decisions to AI, to achieve human-chosen objectives
        \item AI systems should improve, not subvert, the processes of a healthy society
        \item An arms race in LAWS should be avoided
    \end{enumerate}
    \item Long Term Issues:
    \begin{enumerate}
        \item Avoid assumptions on the upper limits of AI
        \item Advanced AI could have a huge impact on life on Earth, and must be managed with care
        \item Risks posed by AI systems must be subject to planning and mitigation systems proportional to their risk
        \item AI designed to improve/replicate themselves must be subject to strict safety and control measures
        \item AI and Superintelligence should only be designed to benefit all of humanity, not just one organisation
    \end{enumerate}
\end{enumerate}